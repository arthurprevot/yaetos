# Jobs details below. Common job params listed at the bottom of this file.
jobs:
  examples/ex0_extraction_job.py:
    api_inputs: {'path': 'https://raw.githubusercontent.com/wikimedia-research/Discovery-Hiring-Analyst-2016/master/events_log.csv.gz'}
    output: {'path':'{base_path}/wiki_example/input/{now}/', 'type':'csv'}

  examples/ex1_full_sql_job.sql:  # shows 100% sql job, easiest when sql is enough
    py_job: 'yaetos/sql_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example_sql/output_ex1_full_sql/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    frequency: 1 day
    start_date: "{today}T07:00:00"
    owners: ['some_email@address.com']

  examples/ex1_frameworked_job.py:  # shows frameworked pyspark ops, same as ex1_full_sql_job but gives access to spark ops to expand on sql.
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex1_frameworked/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    frequency: 1 day
    start_date: "{today}T07:00:00"
    owners: ['some_email@address.com']

  job_using_generic_template:  # to show how to reuse existing job code
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex1_frameworked_p2/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]

  examples/ex1_raw_job.py:
    frequency: 24h
    owners: ['some_email@address.com']

  examples/ex2_frameworked_job.py:  # more complex version of ex1_frameworked_job
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex2/{now}/', 'type':'csv'}
    frequency: 1 day
    start_date: "{today}T07:00:00"
    owners: ['some_email@address.com']

  examples/ex3_incremental_job.py:  # focus on incremental loading and dropping
    inputs:
      processed_events: {'path':"{base_path}/wiki_example/output_ex3_dep/{latest}/", 'type':'csv', 'inc_field': 'timestamp_obj'}
    output: {'path':'{base_path}/wiki_example/output_ex3_inc/incremental_build_v1/', 'type':'csv', 'inc_field': 'other_timestamp'}
    dependencies: [examples/ex3_incremental_prep_job.py]
    frequency: 24h
    owners: ['some_email@address.com']

  examples/ex3_incremental_prep_job.py:  # shows computation of dependency as necessary for ex3_incremental_job
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex3_dep/{now}/', 'type':'csv'}
    frequency: 24h
    owners: ['some_email@address.com']

  examples/ex4_dependency1_job.py:  # shows dependency
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex4_dep1/{now}/', 'type':'csv'}

  examples/ex4_dependency2_job.py:  # shows dependency without specifying inputs path since it is pulled from upstream.
    inputs:
      # some_events: {'from':'examples/ex4_dependency1_job.py'}
      some_events: {'path':'{base_path}/wiki_example/output_ex4_dep1/{latest}/', 'type':'csv', 'from':'examples/ex4_dependency1_job.py'}
    output: {'path':'{base_path}/wiki_example/output_ex4_dep2/{now}/', 'type':'csv'}
    dependencies: [examples/ex4_dependency1_job.py]

  examples/ex4_dependency3_job.sql:  # shows dependency with sql
    py_job: 'yaetos/sql_job.py'
    inputs:
      some_events: {'path':'{base_path}/wiki_example/output_ex4_dep2/{latest}/', 'type':'csv', 'from':'examples/ex4_dependency2_job.py'}  # 'path' not needed when run as dependency
    output: {'path':'{base_path}/wiki_example/output_ex4_dep3/{now}/', 'type':'csv'}
    dependencies: [examples/ex4_dependency2_job.py, examples/ex4_dependency1_job.py]

  examples/ex4_dependency4_job.py:  # shows dependency
    inputs:
      some_events: {'path':'{base_path}/wiki_example/output_ex4_dep3/{latest}/', 'type':'csv', 'from':'examples/ex4_dependency3_job.sql'}
    output: {'path':'{base_path}/wiki_example/output_ex4_dep4/{now}/', 'type':'csv'}
    dependencies: [examples/ex4_dependency3_job.sql]

  examples/ex5_copy_to_oracle_job.py:  # shows frameworked pyspark ops, same as ex1_full_sql_job but gives access to spark ops to expand on sql.
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex5_copy_to_oracle/{now}/', 'type':'csv'}
    copy_to_oracle: {'creds': 'oracle', 'table': 'sandbox.test_ex5_pyspark_job'}

  examples/ex5_copy_to_redshift_job.py:  # shows frameworked pyspark ops, same as ex1_full_sql_job but gives access to spark ops to expand on sql.
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex5_copy_to_oracle/{now}/', 'type':'csv'}
    copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': '{schema}.test_ex5_pyspark_job'}

  examples/ex6_mysql_job.py:  # requires mysql instance running
    api_inputs: {'api_creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{base_path}/mysql_example/output_ex6_mysql/{now}/', 'type':'csv'}

  job_with_no_output:  # shows job with no output (still requiring table as output but not dumped to disk)
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'n/a', 'type':'None'}

  job_with_more_resources:
    description: "To show how to size cluster and machines. See ec2_instance_master and emr_core_instances params below"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex1_frameworked_p3/{now}/', 'type':'csv'}
    ec2_instance_master: 'm5.4xlarge'
    ec2_instance_slaves: 'm5.4xlarge'
    emr_core_instances: 3

  examples/ex7_extraction_small_job.py:  # shows frameworked pyspark ops, same as ex1_full_sql_job but gives access to spark ops to expand on sql.
    inputs:
      events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/input_small_parquet/{now}/', 'type':'parquet'}
    dependencies: [examples/ex0_extraction_job.py]

  examples/ex7_pandas_job.py:
    description: "job loading and processing data with pandas. No spark involved."
    inputs:
      # some_events: {'path':"{base_path}/wiki_example/input_small/{latest}/", 'type':'csv', 'engine':'pandas', 'read_kwargs':'{}'} # works
      # other_events: {'path':"{base_path}/wiki_example/input_small/{latest}/", 'type':'csv', 'engine':'pandas', 'read_kwargs':'{}'} # works
      some_events: {'path':"{base_path}/wiki_example/input_small_parquet/{latest}/", 'type':'parquet', 'engine':'pandas', 'read_kwargs':'{}'}
      other_events: {'path':"{base_path}/wiki_example/input_small_parquet/{latest}/", 'type':'parquet', 'engine':'pandas', 'read_kwargs':'{}'}
    # output: {'path':'{base_path}/wiki_example/output_ex7_pandas/output.csv', 'type':'csv', 'engine':'pandas', 'save_kwargs':"{'sep':'|'}"} # works
    output: {'path':'{base_path}/wiki_example/output_ex7_pandas/output.parquet', 'type':'parquet', 'engine':'pandas', 'save_kwargs':"{}"}
    engine: pandas
    # dependencies: [examples/ex7_extraction_small_job.py]

  examples/ex7_hybrid_pandas_spark_job.py:
    description: "Job processing in pandas, loading and dropping done with spark."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex7_hybrid_pandas_spark/{now}/output.csv', 'type':'csv'}

  examples/ex8_koalas_job.py:
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex8_koalas/{now}/', 'type':'csv'}

  examples/ex9_redshift_job.py:
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex9_redshift/{now}/', 'type':'csv'}
    copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': '{schema}.test_ex9_redshift'}

  examples/ex9_mysql_job.py:
    db_inputs: {'creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{base_path}/wiki_example/output_ex9_mysql/{now}/', 'type':'csv'}

  examples/ex9_mysql_framework_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'type':'mysql', 'db_table': 'some_schema.some_table', 'creds': 'some_mysql_cred_section', 'note':'creds defined in conf/connections.cfg'}
    output: {'path':'{base_path}/db_example/output_ex9_mysql_direct/{now}/', 'type':'csv'}
    load_connectors: all
    enable_redshift_push: True

  examples/ex9_clickhouse_job.py:
    db_inputs: {'creds': 'some_clickhouse_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{base_path}/db_example/output_ex9_clickhouse/{now}/', 'type':'csv'}

  examples/ex9_clickhouse_framework_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'type':'clickhouse', 'db_table': 'some_schema.some_table', 'creds': 'some_clickhouse_cred_section', 'note':'creds defined in conf/connections.cfg'}
    output: {'path':'{base_path}/db_example/output_ex9_clickhouse_direct/{now}/', 'type':'csv'}
    copy_to_clickhouse: {'creds': 'some_clickhouse_cred_section', 'table': 'public.test_push_arthur'}
    load_connectors: all
    enable_redshift_push: True

  examples/wordcount_frameworked_job.py:  # shows raw pyspark rdd ops in framework, same as wordcount_raw_job
    inputs:
      lines: {'path':"{base_path}/wordcount_example/input/sample_text.txt", 'type':'txt'}
    output: {'path':'{base_path}/wordcount_example/output_frameworked/{now}/', 'type':'txt'}
    dependencies: [] # list here if any
    frequency: 24h
    owners: ['some_email@address.com']

  # wordcount_raw_job: #Job exists but doesn't rely on jobs_metadata entries

common_params:
  all_mode_params:
    connection_file:  conf/connections.cfg
    redshift_s3_tmp_dir: s3a://dev-spark/tmp_spark/
    email_cred_section: some_email_cred_section  # Section from "connection_file"
    spark_version: '3.0' # options: '2.4' 'or '3.0'  # TODO: validate it works in EMR
  mode_specific_params:
    prod_EMR:
      base_path: s3://prod-spark  # don't add '/' at the end
      schema: frontroom
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev
      jobs_folder:      jobs/
      load_connectors: all
      enable_redshift_push: True
      save_schemas: False
      manage_git_info: True
    dev_EMR:
      base_path: s3://dev-spark2/yaetos  # don't add '/' at the end
      schema: sandbox
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev
      jobs_folder:      jobs/
      load_connectors: all
      enable_redshift_push: False
      save_schemas: False
      manage_git_info: True
    dev_local:
      base_path: ./data  # don't add '/' at the end
      schema: sandbox
      load_connectors: none
      aws_config_file:  none
      enable_redshift_push: False
      save_schemas: True
      manage_git_info: False
