# Jobs details below. Common job params listed at the bottom of this file.
jobs:
  examples/ex0_extraction_job.py:
    description: "Sample API extraction job, pulling public wikipedia data."
    api_inputs: {'path': 'https://raw.githubusercontent.com/wikimedia-research/Discovery-Hiring-Analyst-2016/master/events_log.csv.gz'}
    output: {'path':'{base_path}/wiki_example/input/{now}/output.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  examples/ex1_sql_job.sql:
    description: "shows sql job, easiest when sql is enough"
    py_job: 'yaetos/sql_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example_sql/output_ex1_sql/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    repartition: 1
    frequency: 1 day
    start_date: "{today}T07:00:00"
    owners: ['some_email@address.com']

  examples/ex1_frameworked_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex1_frameworked/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    frequency: 1 day
    start_date: "{today}T07:00:00"
    owners: ['some_email@address.com']

  job_using_generic_template:
    description: "to show how to reuse existing job code"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex1_frameworked_p2/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]

  examples/ex1_raw_job.py:
    frequency: 24h
    owners: ['some_email@address.com']

  examples/ex2_frameworked_job.py:
    description: "more complex version of ex1_frameworked_job"
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex2/{now}/', 'type':'csv'}
    frequency: 1 day
    start_date: "{today}T07:00:00"
    owners: ['some_email@address.com']

  examples/ex3_incremental_job.py:
    description: "focus on incremental loading and dropping"
    inputs:
      processed_events: {'path':"{base_path}/wiki_example/output_ex3_dep/{latest}/", 'type':'csv', 'inc_field': 'timestamp_obj'}
    output: {'path':'{base_path}/wiki_example/output_ex3_inc/incremental_build_v1/', 'type':'csv', 'inc_field': 'other_timestamp'}
    dependencies: [examples/ex3_incremental_prep_job.py]
    frequency: 24h
    owners: ['some_email@address.com']

  examples/ex3_incremental_prep_job.py:
    description: "shows computation of dependency as necessary for ex3_incremental_job"
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex3_dep/{now}/', 'type':'csv'}
    frequency: 24h
    owners: ['some_email@address.com']

  examples/ex4_dependency1_job.py:
    description: "shows dependency"
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex4_dep1/{now}/', 'type':'csv'}

  examples/ex4_dependency2_job.py:
    description: "shows dependency without specifying inputs path since it is pulled from upstream."
    inputs:
      some_events: {'path':'{base_path}/wiki_example/output_ex4_dep1/{latest}/', 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/wiki_example/output_ex4_dep2/{now}/output.csv', 'df_type':'pandas', 'type':'csv'}
    dependencies: [examples/ex4_dependency1_job.py]

  examples/ex4_dependency3_job.sql:
    description: "shows dependency with sql"
    py_job: 'yaetos/sql_job.py'
    inputs:
      some_events: {'path':'{base_path}/wiki_example/output_ex4_dep2/{latest}/', 'type':'csv', 'from':'examples/ex4_dependency2_job.py'}  # 'path' not needed when run as dependency
    output: {'path':'{base_path}/wiki_example/output_ex4_dep3/{now}/', 'type':'csv'}
    dependencies: [examples/ex4_dependency2_job.py, examples/ex4_dependency1_job.py]

  examples/ex4_dependency4_job.py:
    description: "shows dependency"
    inputs:
      some_events: {'path':'{base_path}/wiki_example/output_ex4_dep3/{latest}/', 'type':'csv', 'from':'examples/ex4_dependency3_job.sql'}
    output: {'path':'{base_path}/wiki_example/output_ex4_dep4/{now}/', 'type':'csv'}
    dependencies: [examples/ex4_dependency3_job.sql]

  examples/ex5_copy_to_oracle_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex5_copy_to_oracle/{now}/', 'type':'csv'}
    copy_to_oracle: {'creds': 'oracle', 'table': 'sandbox.test_ex5_pyspark_job'}

  examples/ex5_copy_to_redshift_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex5_copy_to_oracle/{now}/', 'type':'csv'}
    copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': '{schema}.test_ex5_pyspark_job'}

  examples/ex6_mysql_job.py:
    description: "requires mysql instance running"
    api_inputs: {'api_creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{base_path}/mysql_example/output_ex6_mysql/{now}/', 'type':'csv'}

  job_with_no_output:
    description: "shows job with no output (still requiring table as output but not dumped to disk)"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'n/a', 'type':'None'}

  job_with_more_resources:
    description: "To show how to change the machine specs and size of the cluster. See ec2_instance_master and emr_core_instances params below"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex1_frameworked_p3/{now}/', 'type':'csv'}
    ec2_instance_master: 'm5.4xlarge'
    ec2_instance_slaves: 'm5.4xlarge'
    emr_core_instances: 3

  examples/ex7_pandas_job.py:
    description: "job loading and processing data with pandas. No spark involved."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv', 'df_type':'pandas'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{}}
    output: {'path':'{base_path}/wiki_example/output_ex7_pandas/{now}/output.csv', 'type':'csv', 'df_type':'pandas', 'save_kwargs':{'sep':'|'}}
    spark_boot: False

  examples/ex7_hybrid_pandas_spark_job.py:
    description: "Job processing in pandas, loading and dropping done with spark."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex7_hybrid_pandas_spark/{now}/', 'type':'csv'}

  examples/ex8_koalas_job.py:
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex8_koalas/{now}/', 'type':'csv'}

  examples/ex9_redshift_job.py:
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex9_redshift/{now}/', 'type':'csv'}
    copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': '{schema}.test_ex9_redshift'}

  examples/ex9_mysql_job.py:
    db_inputs: {'creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{base_path}/wiki_example/output_ex9_mysql/{now}/', 'type':'csv'}

  examples/ex9_mysql_framework_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'type':'mysql', 'db_table': 'some_schema.some_table', 'creds': 'some_mysql_cred_section', 'note':'creds defined in conf/connections.cfg'}
    output: {'path':'{base_path}/db_example/output_ex9_mysql_direct/{now}/', 'type':'csv'}
    load_connectors: all
    enable_redshift_push: True

  examples/ex9_clickhouse_job.py:
    db_inputs: {'creds': 'some_clickhouse_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{base_path}/db_example/output_ex9_clickhouse/{now}/', 'type':'csv'}

  examples/ex9_clickhouse_framework_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'type':'clickhouse', 'db_table': 'some_schema.some_table', 'creds': 'some_clickhouse_cred_section', 'note':'creds defined in conf/connections.cfg'}
    output: {'path':'{base_path}/db_example/output_ex9_clickhouse_direct/{now}/', 'type':'csv'}
    copy_to_clickhouse: {'creds': 'some_clickhouse_cred_section', 'table': 'public.test_push_arthur'}
    load_connectors: all
    enable_redshift_push: True

  examples/wordcount_frameworked_job.py:
    description: "shows raw pyspark rdd ops in framework, same as wordcount_raw_job"
    inputs:
      lines: {'path':"{base_path}/wordcount_example/input/sample_text.txt", 'type':'txt'}
    output: {'path':'{base_path}/wordcount_example/output_frameworked/{now}/', 'type':'txt'}
    dependencies: [] # list here if any
    frequency: 24h
    owners: ['some_email@address.com']

  # wordcount_raw_job: #Job exists but doesn't rely on jobs_metadata entries

  marketing/github_accounts_extraction_job.py:
    description: "Github API extraction job, pulling traffic data."
    api_inputs: {'creds': 'github'}
    inputs:
      github_accounts_man: {'path':"./data_man/github_accounts.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/github/users/{now}/output.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  marketing/github_repos_extraction_job.py:
    description: "Github API extraction job, pulling traffic data."
    api_inputs: {'creds': 'github'}
    inputs:
      github_accounts: {'path':"{base_path}/github/users/{latest}/output.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/github/user_repos/{now}/output.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [marketing/github_accounts_extraction_job.py]
    spark_boot: False

  marketing/github_repo_list_extraction_job.py:
    description: "Github API extraction job, pulling traffic data."
    api_inputs: {'creds': 'github'}
    output: {'path':'{base_path}/github/repos/{now}/output.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

# ----- Params -------
common_params:
  all_mode_params:
    connection_file:  conf/connections.cfg
    redshift_s3_tmp_dir: s3a://dev-spark/tmp_spark/
    email_cred_section: some_email_cred_section  # Section from "connection_file"
    spark_version: '3.0' # options: '2.4' 'or '3.0'
  mode_specific_params:
    prod_EMR:
      base_path: s3://prod-spark  # don't add '/' at the end
      schema: frontroom
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev
      jobs_folder:      jobs/
      load_connectors: all
      enable_redshift_push: True
      save_schemas: False
      manage_git_info: True
    dev_EMR:
      base_path: s3://dev-spark  # don't add '/' at the end
      schema: sandbox
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev
      jobs_folder:      jobs/
      load_connectors: all
      enable_redshift_push: False
      save_schemas: False
      manage_git_info: True
    dev_local:
      base_path: ./data  # don't add '/' at the end
      schema: sandbox
      load_connectors: none
      aws_config_file:  none
      enable_redshift_push: False
      save_schemas: True
      manage_git_info: False
