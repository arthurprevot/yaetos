# Jobs details below. Common job params listed at the bottom of this file.
jobs:
  examples/ex0_extraction_job.py:
    description: "Sample API extraction job, pulling public wikipedia data."
    api_inputs: {'path': 'https://raw.githubusercontent.com/wikimedia-research/Discovery-Hiring-Analyst-2016/master/events_log.csv.gz'}
    output: {'path':'{base_path}/wiki_example/input/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    frequency: '@once'
    spark_boot: False

  examples/ex1_sql_job.sql:
    description: "shows sql job, easiest when sql is enough"
    py_job: 'jobs/generic/sql_pandas_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv', 'df_type':'pandas'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/wiki_example_sql/output_ex1_sql_pandas/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [examples/ex0_extraction_job.py]
    spark_boot: False
    frequency: '@daily'
    start_date: '{today}T07:00:00+00:00'
    emails: ['some_email@address.com']

  examples/ex7_pandas_job.py:
    description: "job loading and processing data with pandas. No spark involved."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv', 'df_type':'pandas'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{}}
    output: {'path':'{base_path}/wiki_example/output_ex7_pandas/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas', 'save_kwargs':{'sep':'|'}}
    dependencies: [examples/ex0_extraction_job.py]
    frequency: '@once'
    airflow.default_args.retries: 3
    airflow.default_args.retry_delay: 'timedelta(minutes=5)'
    spark_boot: False

  examples/ex1_sql_spark_job:
    description: "shows sql job, using spark, for large datasets."
    py_job: 'jobs/generic/sql_spark_job.py'
    sql_file: 'jobs/examples/ex1_sql_job.sql'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example_sql/output_ex1_sql_spark/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    repartition: 1
    frequency: '@once'
    emails: ['some_email@address.com']

  examples/ex1_frameworked_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex1_frameworked/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    frequency: 1 day
    start_date: "{today}T07:00:00+00:00"
    emails: ['some_email@address.com']

  job_using_generic_template:
    description: "to show how to reuse existing job code"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex1_frameworked_p2/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]

  examples/ex1_raw_job.py:
    frequency: 24h
    emails: ['some_email@address.com']

  examples/ex2_frameworked_job.py:
    description: "more complex version of ex1_frameworked_job"
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex2/{now}/', 'type':'csv'}
    frequency: 1 day
    start_date: "{today}T07:00:00+00:00"
    emails: ['some_email@address.com']

  examples/ex3_incremental_job.py:
    description: "focus on incremental loading and dropping"
    inputs:
      processed_events: {'path':"{base_path}/wiki_example/output_ex3_dep/{latest}/", 'type':'csv', 'inc_field': 'timestamp_obj'}
    output: {'path':'{base_path}/wiki_example/output_ex3_inc/incremental_build_v1/', 'type':'csv', 'inc_field': 'other_timestamp'}
    dependencies: [examples/ex3_incremental_prep_job.py]
    frequency: 24h
    emails: ['some_email@address.com']

  examples/ex3_incremental_prep_job.py:
    description: "shows computation of dependency as necessary for ex3_incremental_job"
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex3_dep/{now}/', 'type':'csv'}
    frequency: 24h
    emails: ['some_email@address.com']

  examples/ex4_dependency1_job.py:
    description: "shows dependency"
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex4_dep1/{now}/', 'type':'csv'}

  examples/ex4_dependency2_job.py:
    description: "shows dependency without specifying inputs path since it is pulled from upstream."
    inputs:
      some_events: {'path':'{base_path}/wiki_example/output_ex4_dep1/{latest}/', 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/wiki_example/output_ex4_dep2/{now}/dataset.csv', 'df_type':'pandas', 'type':'csv'}
    dependencies: [examples/ex4_dependency1_job.py]

  examples/ex4_dependency3_job.sql:
    description: "shows dependency with sql"
    py_job: 'jobs/generic/sql_pandas_job.py'
    inputs:
      some_events: {'path':'{base_path}/wiki_example/output_ex4_dep2/{latest}/', 'type':'csv', 'df_type':'pandas', 'from':'examples/ex4_dependency2_job.py'}  # 'path' not needed when run as dependency
    output: {'path':'{base_path}/wiki_example/output_ex4_dep3/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [examples/ex4_dependency2_job.py, examples/ex4_dependency1_job.py]
    spark_boot: False

  examples/ex4_dependency4_job.py:
    description: "shows dependency"
    inputs:
      some_events: {'path':'{base_path}/wiki_example/output_ex4_dep3/{latest}/', 'type':'csv', 'from':'examples/ex4_dependency3_job.sql'}
    output: {'path':'{base_path}/wiki_example/output_ex4_dep4/{now}/', 'type':'csv'}
    dependencies: [examples/ex4_dependency3_job.sql]

  examples/ex5_copy_to_oracle_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex5_copy_to_oracle/{now}/', 'type':'csv'}
    copy_to_oracle: {'creds': 'oracle', 'table': 'sandbox.test_ex5_pyspark_job'}

  examples/ex5_copy_to_redshift_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex5_copy_to_oracle/{now}/', 'type':'csv'}
    copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': '{schema}.test_ex5_pyspark_job'}

  examples/ex6_mysql_job.py:
    description: "requires mysql instance running"
    api_inputs: {'api_creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{base_path}/mysql_example/output_ex6_mysql/{now}/', 'type':'csv'}

  job_with_no_output:
    description: "shows job with no output (still requiring table as output but not dumped to disk)"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'n/a', 'type':'None'}

  job_with_more_resources:
    description: "To show how to change the machine specs and size of the cluster. See ec2_instance_master and emr_core_instances params below"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex1_frameworked_p3/{now}/', 'type':'csv'}
    ec2_instance_master: 'm5.4xlarge'
    ec2_instance_slaves: 'm5.4xlarge'
    emr_core_instances: 3

  examples/ex7_pandas_with_sql_job.py:
    description: "job loading and processing data with pandas. No spark involved."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv', 'df_type':'pandas'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{}}
    output: {'path':'{base_path}/wiki_example/output_ex7_pandas_with_sql/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas', 'save_kwargs':{'sep':'|'}}
    spark_boot: False

  examples/ex7_hybrid_pandas_spark_job.py:
    description: "Job processing in pandas, loading and dropping done with spark."
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex7_hybrid_pandas_spark/{now}/', 'type':'csv'}

  examples/ex8_koalas_job.py:
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex8_koalas/{now}/', 'type':'csv'}

  examples/ex9_redshift_job.py:
    inputs:
      some_events: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex9_redshift/{now}/', 'type':'csv'}
    copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': '{schema}.test_ex9_redshift'}

  examples/ex9_mysql_job.py:
    db_inputs: {'creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{base_path}/wiki_example/output_ex9_mysql/{now}/', 'type':'csv'}

  examples/ex9_mysql_framework_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'type':'mysql', 'db_table': 'some_schema.some_table', 'creds': 'some_mysql_cred_section', 'note':'creds defined in conf/connections.cfg'}
    output: {'path':'{base_path}/db_example/output_ex9_mysql_direct/{now}/', 'type':'csv'}
    load_connectors: all
    enable_redshift_push: True

  examples/ex9_clickhouse_job.py:
    db_inputs: {'creds': 'some_clickhouse_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{base_path}/db_example/output_ex9_clickhouse/{now}/', 'type':'csv'}

  examples/ex9_clickhouse_framework_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'type':'clickhouse', 'db_table': 'some_schema.some_table', 'creds': 'some_clickhouse_cred_section', 'note':'creds defined in conf/connections.cfg'}
    output: {'path':'{base_path}/db_example/output_ex9_clickhouse_direct/{now}/', 'type':'csv'}
    copy_to_clickhouse: {'creds': 'some_clickhouse_cred_section', 'table': 'public.test_push_arthur'}
    load_connectors: all
    enable_redshift_push: True

  examples/ex10_excel_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'path':"tests/fixtures/data_sample/wiki_example/input_excel/parts.xlsx", 'type':'xlsx', 'df_type':'pandas', 'read_kwargs':{'engine': 'openpyxl', 'sheet_name': 0, 'header': 1}}  # for xls files, use 'type':'xls' and 'engine': 'xlrd'
    output: {'path':'{base_path}/load_example/excel/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  examples/copy_local_to_cloud_job:
    description: "Copy data from local to cloud. To be executed from local only."
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'path':"tests/fixtures/data_sample/wiki_example/input/", 'type':'csv'}
    base_path: 's3a://mylake-dev/pipelines_data'
    output: {'path':'{base_path}/load_example/test_files/{now}/', 'type':'csv'}
    load_connectors: all

  examples/wordcount_frameworked_job.py:
    description: "shows raw pyspark rdd ops in framework, same as wordcount_raw_job"
    inputs:
      lines: {'path':"{base_path}/wordcount_example/input/sample_text.txt", 'type':'txt'}
    output: {'path':'{base_path}/wordcount_example/output_frameworked/{now}/', 'type':'txt'}
    dependencies: [] # list here if any
    frequency: 24h
    emails: ['some_email@address.com']

  examples/run_jobs:
    description: "shows a job meant to run a bunch of dependencies (dummy job). Takes no input. Generates no output."
    py_job: 'jobs/generic/dummy_job.py'
    spark_boot: False
    dependencies: 
      - examples/ex0_extraction_job.py
      - examples/ex1_sql_job.sql
      - examples/ex7_pandas_job.py

  # wordcount_raw_job: #Job exists but doesn't rely on jobs_metadata entries

  # ----- Marketing Jobs --------
  marketing/github_accounts_extraction_job.py:
    description: "Github API extraction job, pulling account data."
    api_inputs: {'creds': 'github'}
    inputs:
      github_accounts_man: {'path':"./data_manual/github/github_accounts.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/github/users/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  marketing/github_repos_extraction_job.py:
    description: "Github API extraction job, pulling repo data."
    api_inputs: {'creds': 'github'}
    inputs:
      github_accounts: {'path':"{base_path}/github/users/{latest}/dataset.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/github/user_repos/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [marketing/github_accounts_extraction_job.py]
    spark_boot: False

  marketing/github_contributors_extraction_job.py:
    description: "Github API extraction job, pulling contributor data."
    api_inputs: {'creds': 'github'}
    inputs:
      repos: {'path':"{base_path}/github/user_repos/{latest}/dataset.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/github/contributors/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [marketing/github_repos_extraction_job.py]
    spark_boot: False

  marketing/github_committers_extraction_job.py:
    description: "Github API extraction job, pulling committer data, especially email."
    api_inputs: {'creds': 'github'}
    inputs:
      contributors: {'path':"{base_path}/github/contributors/{latest}/dataset.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/github/committers/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [marketing/github_contributors_extraction_job.py]
    spark_boot: False

  marketing/github_repo_list_extraction_job.py:
    description: "Github API extraction job, pulling repo data."
    api_inputs: {'creds': 'github'}
    output: {'path':'{base_path}/github/repos/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  marketing/github_traffic_extraction_job.py:
    description: "Github API extraction job, pulling repo data."
    inputs:
      my_repos: {'path':"./data_manual/github/my_github_repos.csv", 'type':'csv', 'df_type':'pandas'}
    api_inputs: {'creds': 'github'}
    output: {'path':'{base_path}/github/traffic/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  marketing/people_merge_job.py:
    description: "Merge people from linkedin, github..."
    inputs:
      perso: {'path':"./data_manual/people/people.csv", 'type':'csv'}
      linkedin: {'path':"./data_manual/linkedin/export/Connections_mod.csv", 'type':'csv'}
    output: {'path':'{base_path}/people/{now}/', 'type':'csv'}

  # ----- Dashboards--------
  dashboards/wikipedia_demo_dashboard.ipynb:
    description: "Showcase a Dashboard using 'panel', based on dummy wikipedia data (experimental)."
    inputs:
      ex1_sql: {'path':'{base_path}/wiki_example_sql/output_ex1_sql_pandas/{latest}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
      ex7_pandas: {'path':'{base_path}/wiki_example/output_ex7_pandas/{latest}/dataset.csv', 'type':'csv', 'df_type':'pandas', 'read_kwargs':{'sep':'|'}}
    dependencies:
      - examples/ex1_sql_job.sql
      - examples/ex7_pandas_job.py


# ----- Params -------
common_params:
  all_mode_params:
    base_path: '{root_path}/pipelines_data'  # don't add '/' at the end
    s3_dags: '{root_path}/pipelines_metadata/airflow_dags'  # determines which airflow instance to use.
    s3_logs: '{root_path}/pipelines_metadata'
    connection_file:  conf/connections.cfg
    redshift_s3_tmp_dir: s3a://dev-spark/tmp_spark/
    email_cred_section: some_email_cred_section  # Section from "connection_file"
    spark_version: '3.5' # options: '2.4', '3.0', '3.4' or '3.5'
  mode_specific_params:
    prod_EMR:
      root_path: s3://mylake-prod  # don't add '/' at the end
      schema: frontroom
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        pro
      jobs_folder:      jobs/
      load_connectors: all
      enable_redshift_push: True
      save_schemas: False
      manage_git_info: True
    dev_EMR:
      root_path: s3://mylake-dev  # don't add '/' at the end
      schema: sandbox
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev
      jobs_folder:      jobs/
      load_connectors: all
      enable_redshift_push: False
      save_schemas: False
      manage_git_info: True
    dev_local:
      root_path: '.'  # don't add '/' at the end
      base_path: '{root_path}/data'  # don't add '/' at the end
      schema: sandbox
      load_connectors: none
      aws_config_file:  none
      enable_redshift_push: False
      save_schemas: True
      manage_git_info: False
